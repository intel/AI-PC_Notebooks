{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652ea6c8-8d13-4228-853e-fad46db470f5",
   "metadata": {},
   "source": [
    "# IPEX_LLM using Llamacpp on Intel GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0aeac-58b1-4114-95f1-7d3a7a4c34f2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to install IPEX-LLM on Windows with Intel GPUs. It applies to Intel Core Ultra and Core 11 - 14 gen integrated GPUs (iGPUs), as well as Intel Arc Series GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf7db8-9529-47dd-b41d-81b22c8d5848",
   "metadata": {},
   "source": [
    "## What is an AIPC\n",
    "\n",
    "What is an AI PC you ask?\n",
    "\n",
    "Here is an [explanation](https://www.intel.com/content/www/us/en/newsroom/news/what-is-an-ai-pc.htm#gs.a55so1) from Intel:\n",
    "\n",
    "‚ÄùAn AI PC has a CPU, a GPU and an NPU, each with specific AI acceleration capabilities. An NPU, or neural processing unit, is a specialized accelerator that handles artificial intelligence (AI) and machine learning (ML) tasks right on your PC instead of sending data to be processed in the cloud. The GPU and CPU can also process these workloads, but the NPU is especially good at low-power AI calculations. The AI PC represents a fundamental shift in how our computers operate. It is not a solution for a problem that didn‚Äôt exist before. Instead, it promises to be a huge improvement for everyday PC usages.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682eb3e-540b-4814-8142-c54efc32f31b",
   "metadata": {},
   "source": [
    "## Install Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8b6d2-34af-44ad-8363-dea57660bc00",
   "metadata": {},
   "source": [
    "### Step 1: System Preparation\n",
    "\n",
    "To set up your AIPC for running with Intel iGPUs, follow these essential steps:\n",
    "\n",
    "1. Update Intel GPU Drivers: Ensure your system has the latest Intel GPU drivers, which are crucial for optimal performance and compatibility. You can download these directly from Intel's [official website](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html) . Once you have installed the official drivers, you could also install Intel ARC Control to monitor the gpu:\n",
    "\n",
    "   <img src=\"Assets/gpu_arc_control.png\">\n",
    "\n",
    "\n",
    "2. Install Visual Studio 2022 Community edition with C++: Visual Studio 2022, along with the ‚ÄúDesktop Development with C++‚Äù workload, is required. This prepares your environment for C++ based extensions used by the intel SYCL backend that powers accelerated Ollama. You can download VS 2022 Community edition from the official site, [here](https://visualstudio.microsoft.com/downloads/).\n",
    "\n",
    "3. Install conda-forge: conda-forge will manage your Python environments and dependencies efficiently, providing a clean, minimal base for your Python setup. Visit conda-forge's [installation site](https://conda-forge.org/download/) to install for windows.\n",
    "\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8040fd21-7782-4b97-a0eb-327816328f17",
   "metadata": {},
   "source": [
    "## Step 2: Install IPEX-LLM\n",
    "\n",
    "### After installation of conda-forge, open the Miniforge Prompt, and create a new python environment:\n",
    "  ```\n",
    "  conda create -n llm-cpp python=3.11\n",
    "\n",
    "  ```\n",
    "\n",
    "### Activate the new environment\n",
    "```\n",
    "conda activate llm-cpp\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"Assets/llm4.png\">\n",
    "\n",
    "### With the llm-cpp environment active, use pip to install ipex-llm for GPU. \n",
    "\n",
    "```\n",
    "pip install --pre --upgrade ipex-llm[cpp]\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"Assets/llm5.png\">\n",
    "\n",
    "### Create llama-cpp directory\n",
    "\n",
    "```\n",
    "mkdir llama-cpp\n",
    "cd llama-cpp\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"Assets/llm6.png\">\n",
    "\n",
    "### Please run the following command with administrator privilege in Miniforge Prompt. We should see many soft links of llama.cpp‚Äôs executable files in current directory.\n",
    "```\n",
    "init-llama-cpp.bat\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"Assets/llm7.png\">\n",
    "\n",
    "### Set the following environment variables according to your device to use GPU acceleration\n",
    "For Intel iGPU:\n",
    "```\n",
    "set SYCL_CACHE_PERSISTENT=1\n",
    "\n",
    "```\n",
    "### Below shows a simple example to show how to run a community GGUF model with IPEX-LLM\n",
    "* Download and run the model for example as below \n",
    "\n",
    "```\n",
    "main -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -n 32 --prompt \"What is AI\" -t 8 -e -ngl 33 --color\n",
    "```\n",
    "\n",
    "<img src=\"Assets/llm8.png\">\n",
    "\n",
    "### Below is an example output\n",
    "\n",
    "<img src=\"Assets/llm9.png\">\n",
    "\n",
    "\n",
    "<img src=\"Assets/llm10.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5cd95-18a4-4879-9d3d-05e302448ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! C:\\workshop\\llama-cpp\\main.exe -m ../models/llama-2-7b-chat.Q5_K_M.gguf -n 100 --prompt \"What is AI\" -t 16 -ngl 999 --color -e "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec180ac3-e74a-41d9-a9b9-65478dcea556",
   "metadata": {},
   "source": [
    "## Complete code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b94504-fcc8-454f-8a8d-b7312b7c0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/st_ipexllm_native.py\n",
    "import streamlit as st\n",
    "import subprocess\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "st.title(\"Chat with me!\")\n",
    "\n",
    "# Get the inputs from the text fields with required logs\n",
    "exe_path = st.text_input(\"Enter the path to the main.exe binary generated by the steps outlined:\",value=\"..\\llama-cpp\\main.exe\", key=\"exe_path\")\n",
    "print(f\"{exe_path}\\n\")\n",
    "if exe_path:\n",
    "    if os.path.exists(exe_path):\n",
    "        if os.path.isfile(exe_path):\n",
    "            print(f\"valid file path: {exe_path}\")\n",
    "        else:\n",
    "            st.error(f\"The path {exe_path} is not a file\")\n",
    "    else:\n",
    "        st.error(f\"The path {exe_path} does not exist\")\n",
    "else:\n",
    "    print(\"Please enter the file path\")\n",
    "\n",
    "model_path = st.text_input(\"Enter model file path:\", value=\"..\\models\\llama-2-7b-chat.Q5_K_M.gguf\", key=\"model_name\")\n",
    "print(f\"{model_path}\\n\")\n",
    "if model_path:\n",
    "    if os.path.exists(model_path):\n",
    "        if os.path.isfile(model_path):\n",
    "            print(f\"valid file path: {model_path}\")\n",
    "        else:\n",
    "            st.error(f\"The path {model_path} is not a file\")\n",
    "    else:\n",
    "        st.error(f\"The path {model_path} does not exist\")\n",
    "else:\n",
    "    print(\"Please enter the file path\")\n",
    "\n",
    "\n",
    "num_words = st.text_input(\"Enter the number of words you'd expect to see in your answer:\", value=\"100\", key=\"num_words\")\n",
    "print(f\"{num_words}\\n\")\n",
    "\n",
    "question = st.text_input(\"Enter your question\", value=\"What is AI\", key=\"question\")\n",
    "question = f'\"{question}\"'\n",
    "print(f\"{question}\\n\")\n",
    "num_cores = st.text_input(\"Enter the number of cores\", value=\"16\", key=\"num_cores\")\n",
    "print(f\"{num_cores}\\n\")\n",
    " \n",
    "gpu_layers = st.text_input(\"Enter number of GPU layers:\", value=\"999\", key=\"gpu_layers\")\n",
    "print(f\"{gpu_layers}\\n\")\n",
    "\n",
    "def stdout_typewriter_effect(stdout_container, current_stdout):\n",
    "    current_char = \"\"\n",
    "    for char in current_stdout:\n",
    "        current_char+=char\n",
    "        stdout_container.markdown(current_char)\n",
    "        time.sleep(0.01)\n",
    "\n",
    "def launch_exe():\n",
    "    stdout_chunks = []\n",
    "    stderr_llama_time = []\n",
    "    \n",
    "    def append_stdout(pipe, stdout_lines):\n",
    "        for line in iter(pipe.readline, ''):\n",
    "            if line:\n",
    "                print(line.strip())\n",
    "                stdout_lines.append(line.strip())\n",
    "        pipe.close()\n",
    "\n",
    "    def append_stderr(pipe, stderr_lines):\n",
    "        for line in iter(pipe.readline, ''):\n",
    "            if line.startswith(\"llama_print_timings\"):\n",
    "                print(line.strip())\n",
    "                stderr_lines.append(line.strip())\n",
    "        pipe.close()\n",
    "\n",
    "    filter_command = '| findstr \"^\"'\n",
    "    # command to run    \n",
    "    commandparams = exe_path + \" \" + \"-m\" + \" \" + model_path + \" \" + \"-n \" + \" \" + num_words + \" \" + \"--prompt \" + \" \" + question + \" \" +  \"-t \" + \" \" + num_cores + \" \" + \"-e -ngl\" + \" \" + gpu_layers + \" \" + filter_command\n",
    "    # logging command for easy debugging\n",
    "    print(f\"{commandparams}\")\n",
    "    try:\n",
    "        # Use subprocess.Popen() to execute the EXE file with command-line parameters and capture the output in real-time\n",
    "        result = subprocess.Popen(commandparams, shell=True, stdout=subprocess.PIPE, stderr = subprocess.PIPE, text=True)\n",
    "\n",
    "        stdout_thread = threading.Thread(target=append_stdout, args=(result.stdout, stdout_chunks))\n",
    "        stderr_thread = threading.Thread(target=append_stderr, args=(result.stderr, stderr_llama_time))\n",
    "        stdout_thread.start()\n",
    "        stderr_thread.start()\n",
    "        stdout_container = st.empty()\n",
    "        stderr_container = st.empty()\n",
    "\n",
    "        # result.poll() returns None only if the subprocess is still running otherwise it returns the return code of subprocess\n",
    "        # this method is not waiting for subprocess to complete as it only checks for the current status   \n",
    "        while result.poll() is None and stdout_thread.is_alive or stderr_thread.is_alive():\n",
    "            # stdout_container.markdown('\\n'.join(stdout_lines))\n",
    "            stdout_typewriter_effect(stdout_container, '\\n'.join(stdout_chunks))\n",
    "            stderr_container.text('\\n'.join(stderr_llama_time))\n",
    "            stdout_thread.join(timeout=0.1)\n",
    "            stderr_thread.join(timeout=0.1)\n",
    "            \n",
    "        stdout_thread.join()\n",
    "        stderr_thread.join()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        st.error(\"The specified EXE file does not exist.\")\n",
    "    \n",
    "if st.button(\"Generate\"):\n",
    "    with st.spinner(\"Running....Please wait..üêé\"): \n",
    "        launch_exe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d3f11-c86f-4971-ad9e-4562eb76b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run src/st_ipexllm_native.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e741522-23ff-41e9-8bc0-59e0ea126069",
   "metadata": {},
   "source": [
    "### Streamlit sample output\n",
    "\n",
    "Below is the output of a sample run from the streamlit application and offloaded to iGPU\n",
    "\n",
    "<img src=\"Assets/llm11.png\"> <img src=\"Assets/output2.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92387fa9-2376-49a7-a94b-a29f254a0471",
   "metadata": {},
   "source": [
    "* Reference: https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac73234-1851-42ad-9b6c-67ba9562db32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-cpp",
   "language": "python",
   "name": "llm-cpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
