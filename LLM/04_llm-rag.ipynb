{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02a561f4",
   "metadata": {},
   "source": [
    "# Create a RAG system on AIPC using Ollama\n",
    "\n",
    "## Introduction  \n",
    "\n",
    "This notebook demonstrates how to run LLM inference for a Retrieval-Augmented Generation (RAG) application using Ollama locally on an AI PC. It is optimized for Intel¬Æ Core‚Ñ¢ Ultra processors, utilizing the combined capabilities of the CPU, GPU, and NPU for efficient AI workloads. \n",
    "\n",
    "### What is an AI PC?  \n",
    "\n",
    "An AI PC is a next-generation computing platform equipped with a CPU, GPU, and NPU, each designed with specific AI acceleration capabilities.  \n",
    "\n",
    "- **Fast Response (CPU)**  \n",
    "  The central processing unit (CPU) is optimized for smaller, low-latency workloads, making it ideal for quick responses and general-purpose tasks.  \n",
    "\n",
    "- **High Throughput (GPU)**  \n",
    "  The graphics processing unit (GPU) excels at handling large-scale workloads that require high parallelism and throughput, making it suitable for tasks like deep learning and data processing.  \n",
    "\n",
    "- **Power Efficiency (NPU)**  \n",
    "  The neural processing unit (NPU) is designed for sustained, heavily-used AI workloads, delivering high efficiency and low power consumption for tasks like inference and machine learning.  \n",
    "\n",
    "The AI PC represents a transformative shift in computing, enabling advanced AI applications like LLM-based RAG workflows to run seamlessly on local hardware. This innovation enhances everyday PC usage by delivering faster, more efficient AI experiences without relying on cloud resources.  \n",
    "\n",
    "In this notebook, we‚Äôll explore how to use the AI PC‚Äôs capabilities to perform LLM inference and integrate it into a RAG pipeline, showcasing the power of local AI acceleration for modern applications. \n",
    "\n",
    "**Retrieval-augmented generation (RAG)** is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model‚Äôs cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fb8b0e4",
   "metadata": {},
   "source": [
    "## Run QA over Document\n",
    "\n",
    "Now, when model created, we can setup Chatbot interface using Streamlit\n",
    "\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "- **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happen offline.\n",
    "\n",
    "- **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "**Indexing**\n",
    "\n",
    "1. `Load`: First we need to load our data. We‚Äôll use DocumentLoaders for this.\n",
    "2. `Split`: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won‚Äôt in a model‚Äôs finite context window.\n",
    "3. `Store`: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "![Indexing pipeline](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/dfed2ba3-0c3a-4e0e-a2a7-01638730486a)\n",
    "\n",
    "**Retrieval and generation**\n",
    "\n",
    "1. `Retrieve`: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "2. `Generate`: A LLM produces an answer using a prompt that includes the question and the retrieved data.\n",
    "\n",
    "![Retrieval and generation pipeline](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/f0545ddc-c0cd-4569-8c86-9879fdab105a)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "602f8ebd-789c-4eb2-b54d-b23d8f1d8e7b",
   "metadata": {},
   "source": [
    "We can build a RAG pipeline of LangChain through [`create_retrieval_chain`](https://python.langchain.com/docs/modules/chains/), which will help to create a chain to connect RAG components including:\n",
    "\n",
    "- [`Vector stores`](https://python.langchain.com/docs/modules/data_connection/vectorstores/)Ôºå\n",
    "- [`Retrievers`](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "- [`LLM`](https://python.langchain.com/docs/integrations/llms/)\n",
    "- [`Embedding`](https://python.langchain.com/docs/integrations/text_embedding/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc6ae1-3321-4a10-83a8-fb4169516391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_community import document_loaders, embeddings, vectorstores, llms\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain import chains, text_splitter, PromptTemplate\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "VECTOR_DB_DIR = \"vector_dbs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179e7c1-8152-4d5c-b2a2-b5aaff8773bd",
   "metadata": {},
   "source": [
    "### Document Loaders in RAG\n",
    "\n",
    "* Document loaders in RAG are used to load and preprocess the documents that will be used for retrieval during the question answering process.\n",
    "* Document loaders are responsible for preprocessing the documents. This includes tokenizing the text, converting it to the format expected by the retriever, and creating batches of documents.\n",
    "* Document loaders work in conjunction with the retriever in RAG. The retriever uses the documents loaded by the document loader to find the most relevant documents for a given query.\n",
    "* The WebBaseLoader in Retrieval Augmented Generation (RAG) is a type of document loader that is designed to load documents from the web.\n",
    "* The WebBaseLoader is used when the documents for retrieval are not stored locally or in a Hugging Face dataset, but are instead located on the web. This can be useful when you want to use the most up-to-date information available on the internet for your question answering system\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29cec6-0ec9-4074-9068-b12fb4be5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(url):\n",
    "    print(\"Loading document from URL...\")\n",
    "    loader = document_loaders.WebBaseLoader(url)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a5840-ac75-448d-b3d8-f86a40fe8a73",
   "metadata": {},
   "source": [
    "### Text splitter\n",
    "\n",
    "* RecursiveCharacterTextSplitter is used to split text into smaller pieces recursively at the character level. \n",
    "* split_documents fuctions splits larger documents into smaller chunks, for easier processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488b84c-830c-41e9-96a4-5fcf728ad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document(text, chunk_size=3000, overlap=200):\n",
    "    print(\"Splitting document into chunks...\")\n",
    "    text_splitter_instance = text_splitter.RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    return text_splitter_instance.split_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc257b5f-1e36-4273-9b41-4c45c2b21106",
   "metadata": {},
   "source": [
    "### Huggingface emdeggings\n",
    "In Retrieval Augmented Generation (RAG) embeddings play a crucial role in the retrieval of relevant documents for a given query.\n",
    "\n",
    "* In RAG, each document in the knowledge base is represented as a dense vector, also known as an embedding. These embeddings are typically generated by a transformer model.\n",
    "* When a query is received, it is also converted into an embedding using the same transformer model. This ensures that the query and the documents are in the same vector space, making it possible to compare them.\n",
    "* Retrieval: The retrieval step in RAG involves finding the documents whose embeddings are most similar to the query embedding. This is typically done using a nearest neighbor search.\n",
    "\n",
    "#### Sentence transformers\n",
    "\n",
    "* You can use a Sentence Transformer to generate embeddings for each document in your knowledge base. Since Sentence Transformers are designed to capture the semantic meaning of sentences, these embeddings should do a good job of representing the content of the documents.\n",
    "* You can also use a Sentence Transformer to generate an embedding for the query. This ensures that the query and the documents are in the same vector space, making it possible to compare them.\n",
    "* By using Sentence Transformers, you can potentially improve the quality of the retrieval step in RAG. Since Sentence Transformers are designed to capture the semantic meaning of sentences, they should be able to find documents that are semantically relevant to the query, even if the query and the documents do not share any exact words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55af744-4ffd-4dfd-b85a-aa5ff401b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_fn(embedding_type=\"huggingface\", model_name=\"sentence-transformers/all-MiniLM-l6-v2\"):\n",
    "    print(f\"Initializing {embedding_type} model with {model_name}...\")\n",
    "    if embedding_type == \"ollama\":\n",
    "        model_name = chat_model\n",
    "        return embeddings.OllamaEmbeddings(model=model_name, base_url=OLLAMA_BASE_URL)\n",
    "    elif embedding_type == \"huggingface\":\n",
    "        model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "        return embeddings.HuggingFaceEmbeddings(model_name=model_name)\n",
    "    elif embedding_type == \"nomic\":\n",
    "        return embeddings.NomicEmbeddings(model_name=model_name)\n",
    "    elif embedding_type == \"fastembed\":\n",
    "        return  FastEmbedEmbeddings(threads=16)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding type: {embedding_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d4ccf-fd68-4380-9a98-01de673e83c9",
   "metadata": {},
   "source": [
    "### Create and get embeddings using ChromaDB\n",
    "\n",
    "Here's a flow chart that describes how embeddings work in a RAG model with ChromaDB:\n",
    "\n",
    "* Query Input: The user inputs a query.\n",
    "* Query Embedding: The query is passed through a transformer-based encoder (like BERT or RoBERTa) to generate a query embedding.\n",
    "* Document Embedding: Each document in the ChromaDB is also passed through a transformer-based encoder to generate a document embedding. This is typically done offline and the embeddings are stored in the database for efficient retrieval.\n",
    "* Embedding Comparison: The query embedding is compared with each document embedding in the ChromaDB. This is done by calculating the cosine similarity or dot product between the query embedding and each document embedding.\n",
    "* Document Retrieval: The documents with the highest similarity scores are retrieved. The number of documents retrieved is a hyperparameter that can be tuned.\n",
    "* Answer Generation: The retrieved documents and the query are passed to a sequence-to-sequence model (like BART or T5) to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b872757-5f72-45e3-ab86-de4edae8af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_embeddings(document_url, embedding_fn, persist_dir=VECTOR_DB_DIR):\n",
    "    vector_store_path = os.path.join(os.getcwd(), persist_dir)    \n",
    "    if os.path.exists(vector_store_path):\n",
    "        print(\"Loading existing vector store...\")\n",
    "        return vectorstores.Chroma(persist_directory=persist_dir, embedding_function=embedding_fn)\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        print(\"No existing vector store found. Creating new one...\")\n",
    "        document = load_document(document_url)\n",
    "        documents = split_document(document)\n",
    "        vector_store = vectorstores.Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embedding_fn,\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "        vector_store.persist()\n",
    "        print(f\"Embedding time: {time.time() - start_time:.2f} seconds\")\n",
    "        return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c806373-8179-4b95-adfb-8eebbb613baf",
   "metadata": {},
   "source": [
    "### Retrievers\n",
    "\n",
    "* Retrievers are responsible for fetching relevant documents from a document store or knowledge base given a query. The retrieved documents are then used by the generator to produce a response.\n",
    "* RetrievalQA is a type of question answering system that uses a retriever to fetch relevant documents given a question, and then uses a reader to extract the answer from the retrieved documents.\n",
    "* RetrievalQA can be seen as a two-step process:\n",
    "    * Retrieval: The retriever fetches relevant documents from the document store given a query.    \n",
    "    * Generation: The generator uses the retrieved documents to generate a response.\n",
    "* This two-step process allows RAG to leverage the strengths of both retrieval-based and generation-based approaches to question answering. The retriever allows RAG to efficiently search a large document store, while the generator allows RAG to generate detailed and coherent responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92cbb3-a9be-4d9a-810f-92b8e43105df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_user_interaction(vector_store, chat_model):\n",
    "    prompt_template = \"\"\"\n",
    "    Use the following pieces of context to answer the question at the end. \n",
    "    If you do not know the answer, answer 'I don't know', limit your response to the answer and nothing more. \n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    qachain = chains.RetrievalQA.from_chain_type(llm=chat_model, retriever=retriever, chain_type=\"stuff\", chain_type_kwargs=chain_type_kwargs)\n",
    "    qachain.invoke({\"query\": \"what is this about?\"})\n",
    "    print(f\"Model warmup complete...\")\n",
    "    while True:\n",
    "        question = input(\"Enter your question (or 'quit' to exit): \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        start_time = time.time()\n",
    "        answer = qachain.invoke({\"query\": question})\n",
    "        print(f\"Answer: {answer['result']}\")\n",
    "        print(f\"Response time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc1652-788e-4f1b-9684-313a82cb25f3",
   "metadata": {},
   "source": [
    "### Run the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cd97b-9aec-4d59-a97e-c2bab0767336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(document_url, embedding_type, chat_model):\n",
    "    embedding_fn = initialize_embedding_fn(embedding_type)\n",
    "    vector_store = get_or_create_embeddings(document_url, embedding_fn)\n",
    "    chat_model_instance = llms.Ollama(base_url=OLLAMA_BASE_URL, model=chat_model)\n",
    "    handle_user_interaction(vector_store, chat_model_instance)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    document_url = \"https://www.gutenberg.org/files/1727/1727-h/1727-h.htm\"    \n",
    "    embedding_type = \"huggingface\"\n",
    "    chat_model = \"llama3:latest\"\n",
    "    main(document_url, embedding_type, chat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0da830-f207-4035-8d63-bb6f4884b4a6",
   "metadata": {},
   "source": [
    "### Streamlit Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33911e-84ab-4346-92a4-3ead03f9d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/st_rag_chromadb.py\n",
    "import streamlit as st\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import ollama\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_community import document_loaders, embeddings, vectorstores, llms\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain import chains, text_splitter, PromptTemplate\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "VECTOR_DB_DIR = \"vector_dbs\"\n",
    "\n",
    "st.header(\"LLM Rag üêª‚Äç‚ùÑÔ∏è\")\n",
    "\n",
    "\n",
    "models = [model[\"name\"] for model in ollama.list()[\"models\"]]\n",
    "model = st.selectbox(\"Choose a model from the list\", models)\n",
    "\n",
    "# Input text to load the document\n",
    "url_path = st.text_input(\"Enter the URL to load for RAG:\",value=\"https://www.gutenberg.org/files/1727/1727-h/1727-h.htm\", key=\"url_path\")\n",
    "\n",
    "# Select embedding type\n",
    "embedding_type = st.selectbox(\"Please select an embedding type\", (\"ollama\", \"huggingface\", \"nomic\", \"fastembed\"),index=1)\n",
    "\n",
    "# Input for RAG\n",
    "question = st.text_input(\"Enter the question for RAG:\", value=\"What is this about\", key=\"question\")\n",
    "\n",
    "## Load the document using document_loaders\n",
    "def load_document(url):\n",
    "    print(\"Loading document from URL...\")\n",
    "    st.markdown(''' :green[Loading document from URL...] ''')\n",
    "    loader = document_loaders.WebBaseLoader(url)\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "## Split the document into multiple chunks\n",
    "def split_document(text, chunk_size=3000, overlap=200):\n",
    "    print(\"Splitting document into chunks...\")\n",
    "    st.markdown(''' :green[Splitting document into chunks...] ''')\n",
    "    text_splitter_instance = text_splitter.RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    return text_splitter_instance.split_documents(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Initialize embeddings for these chunks of data. we can use one of the below four embedding types\n",
    "\n",
    "def initialize_embedding_fn(embedding_type=\"huggingface\", model_name=\"sentence-transformers/all-MiniLM-l6-v2\"):\n",
    "    print(f\"Initializing {embedding_type} model with {model_name}...\")\n",
    "    st.write(f\"Initializing {embedding_type} model with {model_name}...\")\n",
    "    if embedding_type == \"ollama\":\n",
    "        model_name = chat_model\n",
    "        return embeddings.OllamaEmbeddings(model=model_name, base_url=OLLAMA_BASE_URL)\n",
    "    elif embedding_type == \"huggingface\":\n",
    "        model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "        return embeddings.HuggingFaceEmbeddings(model_name=model_name)\n",
    "    elif embedding_type == \"nomic\":\n",
    "        return embeddings.NomicEmbeddings(model_name=model_name)\n",
    "    elif embedding_type == \"fastembed\":\n",
    "        return  FastEmbedEmbeddings(threads=16)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding type: {embedding_type}\")\n",
    "    \n",
    "## Create embeddings for these chunks of data and store it in chromaDB\n",
    "\n",
    "def get_or_create_embeddings(document_url, embedding_fn, persist_dir=VECTOR_DB_DIR):\n",
    "    vector_store_path = os.path.join(os.getcwd(), persist_dir)    \n",
    "    start_time = time.time()\n",
    "    print(\"No existing vector store found. Creating new one...\")\n",
    "    st.markdown(''' :green[No existing vector store found. Creating new one......] ''')\n",
    "    document = load_document(document_url)\n",
    "    documents = split_document(document)\n",
    "    vector_store = vectorstores.Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_fn,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    vector_store.persist()\n",
    "    print(f\"Embedding time: {time.time() - start_time:.2f} seconds\")\n",
    "    st.write(f\"Embedding time: {time.time() - start_time:.2f} seconds\")\n",
    "    return vector_store\n",
    "# Create the user prompt and generate the response\n",
    "def handle_user_interaction(vector_store, chat_model):\n",
    "    prompt_template = \"\"\"\n",
    "    Use the following pieces of context to answer the question at the end. \n",
    "    If you do not know the answer, answer 'I don't know', limit your response to the answer and nothing more. \n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    # Use retrievers to retrieve the data from the database\n",
    "    st.markdown(''' :green[Using retrievers to retrieve the data from the database...] ''')\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    st.markdown(''' :green[Answering the query...] ''')\n",
    "    qachain = chains.RetrievalQA.from_chain_type(llm=chat_model, retriever=retriever, chain_type=\"stuff\", chain_type_kwargs=chain_type_kwargs)\n",
    "    qachain.invoke({\"query\": \"what is this about?\"})\n",
    "    print(f\"Model warmup complete...\")\n",
    "    st.markdown(''' :green[Model warmup complete...] ''')\n",
    "       \n",
    "    \n",
    "          \n",
    "    start_time = time.time()\n",
    "    answer = qachain.invoke({\"query\": question})\n",
    "    print(f\"Answer: {answer['result']}\")    \n",
    "    print(f\"Response time: {time.time() - start_time:.2f} seconds\")\n",
    "    st.write(f\"Response time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    \n",
    "    return answer['result']\n",
    "  \n",
    "       \n",
    "\n",
    "# Main Function to load the document, initialize the embeddings , create the vector database and invoke the model\n",
    "def getfinalresponse(document_url, embedding_type, chat_model):    \n",
    "    \n",
    "    document_url = url_path    \n",
    "    chat_model = model\n",
    "                \n",
    "    embedding_fn = initialize_embedding_fn(embedding_type)\n",
    "    vector_store = get_or_create_embeddings(document_url, embedding_fn)     \n",
    "    chat_model_instance = llms.Ollama(base_url=OLLAMA_BASE_URL, model=chat_model)\n",
    "    return handle_user_interaction(vector_store, chat_model_instance)\n",
    "\n",
    "    \n",
    "submit=st.button(\"Generate\")\n",
    "\n",
    "\n",
    "# generate response\n",
    "if submit:    \n",
    "    document_url = url_path    \n",
    "    chat_model = model\n",
    "    \n",
    "    with st.spinner(\"Loading document....üêé\"):        \n",
    "        st.write(getfinalresponse(document_url, embedding_type, chat_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336754e-43ed-4818-b31c-040847c916d2",
   "metadata": {},
   "source": [
    "### Streamlit Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b5a84-bebf-49b9-b2fa-5e788ed2cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run src/st_rag_chromadb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e0683-569f-45a1-938d-017f03eb8cd9",
   "metadata": {},
   "source": [
    "### Streamlit sample output\n",
    "\n",
    "Below is the output of a sample run from the streamlit application and offloaded to iGPU\n",
    "\n",
    "<img src=\"Assets/rag2.png\"> <img src=\"Assets/rag1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61940d35-bd5b-4199-98a7-2f7d845e7e50",
   "metadata": {},
   "source": [
    "### References\n",
    "https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-agent-langchain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/304aa048-f10c-41c6-bb31-6d2bfdf49cf5",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
