# WebLLM - Chat Application

This project demonstrates how to use WebLLM, a high-performance in-browser LLM inference engine. The examples provided showcase various functionalities and UI implementations to help you get started with integrating WebLLM into your own projects.

All the code for interactivity and LLM inference can be found in [chat.js](./chat.js) file.

## Screenshot

![image](./webll-chatui.png)
